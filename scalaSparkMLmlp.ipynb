{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "name": "scala"
        }, 
        "kernelspec": {
            "name": "scala", 
            "language": "scala", 
            "display_name": "Scala 2.10 with Spark 1.6"
        }
    }, 
    "cells": [
        {
            "source": "## Init SQL context for future use", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\nimport sqlContext.implicits._", 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Ex 1 : Load data and run sample a MLP", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## First we need to load the file from object storage", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "// Sets hadoop config with given credentials - this need to be done only once per container\ndef set_hadoop_config_with_credentials(creds:scala.collection.mutable.HashMap[String, String]){\n    val prefix = \"fs.swift.service.spark\"\n    val hconf = sc.hadoopConfiguration\n    hconf.set(prefix + \".auth.url\", creds(\"auth_url\")+\"/v3/auth/tokens\")\n    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n    hconf.set(prefix + \".tenant\", creds(\"project_id\"))\n    hconf.set(prefix + \".username\", creds(\"user_id\"))\n    hconf.set(prefix + \".password\", creds(\"password\"))\n    hconf.setInt(prefix + \".http.port\", 8080)\n    hconf.set(prefix + \".region\", creds(\"region\"))\n    hconf.setBoolean(prefix + \".public\", true)\n}\n//Build swift url, need to be done for each file\ndef build_swift_url(creds:scala.collection.mutable.HashMap[String, String]):String={\n    \"swift://\"+creds(\"container\")+\".spark/\"+creds(\"filename\")\n}", 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "var credentials_1 = scala.collection.mutable.HashMap[String, String](\n  \"auth_url\"->\"https://identity.open.softlayer.com\",\n  \"project\"->\"object_storage_xxx\",\n  \"project_id\"->\"123654\",\n  \"region\"->\"dallas\",\n  \"user_id\"->\"abcde54\",\n  \"domain_id\"->\"azd564\",\n  \"domain_name\"->\"azd654\",\n  \"username\"->\"member_azd564\",\n  \"password\"->\"\"\"654654\"\"\",\n  \"container\"->\"MyProject\",\n  \"tenantId\"->\"undefined\",\n  \"filename\"->\"sample_multiclass_classification_data.txt\"\n)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "set_hadoop_config_with_credentials(credentials_1)\nval testFile = sc.textFile(build_swift_url(credentials_1))\ntestFile.count()\ntestFile.take(5)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Now we play the sample MLP from\nhttps://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Accuracy: 0.9019607843137255\n"
                }
            ], 
            "source": "//Sample from\n// https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\n// Load the data stored in LIBSVM format as a DataFrame.\nval data = spark.read.format(\"libsvm\").load(build_swift_url(credentials_1))\n//format is: class in1:value in2:value in3:value in4:value\n//1 1:-0.722222 2:-0.166667 3:-0.864407 4:-0.833333 \n//1 1:-0.722222 2:0.166667 3:-0.694915 4:-0.916667 \n//0 1:0.166667 2:-0.416667 3:0.457627 4:0.5 \n//1 1:-0.833333 3:-0.864407 4:-0.916667 \n\n// Split the data into train and test\nval splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\nval train = splits(0)\nval test = splits(1)\n// specify layers for the neural network:\n// input layer of size 4 (features), two intermediate of size 5 and 4\n// and output of size 3 (classes)\nval layers = Array[Int](4, 5, 4, 3)\n// create the trainer and set its parameters\nval trainer = new MultilayerPerceptronClassifier().\n  setLayers(layers).\n  setBlockSize(128).\n  setSeed(1234L).\n  setMaxIter(100)\n// train the model\nval model = trainer.fit(train)\n// compute accuracy on the test set\nval result = model.transform(test)\nval predictionAndLabels = result.select(\"prediction\", \"label\")\nval evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\nprintln(\"Accuracy: \" + evaluator.evaluate(predictionAndLabels))", 
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Ex 2 : Let's try with our own dataset : handwritten numbers from mnist\nhttp://yann.lecun.com/exdb/mnist/\n## Load the training files train-images.idx3-ubyte and train-labels.idx1-ubyte", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "//limits\nval trainSetSize=60000 //max:60000\nval testSetSize=10000 //max:10000", 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "val credentials_train_images = scala.collection.mutable.HashMap[String, String](\n  \"auth_url\"->\"https://identity.open.softlayer.com\",\n  \"project\"->\"object_storage_fxxxx4\",\n  \"project_id\"->\"xxx\",\n  \"region\"->\"dallas\",\n  \"user_id\"->\"xxx\",\n  \"domain_id\"->\"xxx\",\n  \"domain_name\"->\"xxx\",\n  \"username\"->\"member_5xxx4\",\n  \"password\"->\"\"\"xxx\"\"\",\n  \"container\"->\"FirstProject\",\n  \"tenantId\"->\"undefined\",\n  \"filename\"->\"train-images.idx3-ubyte\"\n)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 4, 
                    "data": {
                        "text/plain": "swift://FirstProject.spark/train-images.idx3-ubyte"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "set_hadoop_config_with_credentials(credentials_train_images)//This need to be done only once per container\nval sFilePathTrainImages = build_swift_url(credentials_train_images)\nsFilePathTrainImages", 
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "Images are 28x28 pixels (unsigned byte) -> 784 bytes, \nThe training file contains 60k images of 784 pixels, \nThe training file size is 784x60000=47040000 bytes + header:4x32bit integers=4*2bytes=16 bytes, \ntotal:47040016", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "//Read image file - binary\n//see http://stackoverflow.com/questions/33654135/reading-binary-file-in-spark-scala\n//v1 - This is not super fancy, but should work\ndef loadidxFile(sFilePath:String,colPrefix:String,iHeaderLength:Int,iRecordLength:Int,iRecordNum:Int)={\n    val binStream = sc.binaryFiles(sFilePath, 1).first._2.open()\n    binStream.skipBytes(iHeaderLength)\n    val records = new Array[Array[Double]](iRecordNum)\n    for(r <- 0 to iRecordNum-1){\n        val record = new Array[Double](iRecordLength+1)//id+pixels\n        record(0)=r\n        for(p <- 1 to iRecordLength){\n          record(p)=binStream.readUnsignedByte().toDouble\n        }        \n        records(r)=record\n    }\n    //we now have an array of array, lets transform to a dataframe\n    val arrayRDD = sc.parallelize(records)\n    //  Generate the schema based on the number of fields\n    import org.apache.spark.sql.types._\n    val fields = StructField(\"id\", DoubleType, nullable = false)::(0 until iRecordLength).toList.map(i => StructField(colPrefix+i, DoubleType, nullable = true))\n    val schema = StructType(fields)\n    import org.apache.spark.sql._\n    sqlContext.createDataFrame(arrayRDD.map(s => Row.fromSeq(s.toSeq)),schema)\n}\n\nval trainImagesDF=loadidxFile(sFilePathTrainImages,\"p\",16,784,trainSetSize)// 60000\ntrainImagesDF.count()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Let's preview a digit of the training set", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 6, 
                    "data": {
                        "text/plain": "[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,18,18,18,126,136,175,26,166,255,247,127,0,0,0,0,0,0,0,0,0,0,0,0,30,36,94,154,170,253,253,253,253,253,225,172,253,242,195,64,0,0,0,0,0,0,0,0,0,0,0,49,238,253,253,253,253,253,253,253,253,251,93,82,82,56,39,0,0,0,0,0,0,0,0,0,0,0,0,18,219,253,253,253,253,253,198,182,247,241,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,156,107,253,253,205,11,0,43,154,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,154,253,90,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,139,253,..."
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "val testRecord = trainImagesDF.take(1)(0)\ntestRecord", 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+---+-----+\n|  x|  y|color|\n+---+---+-----+\n|  0|  0|    0|\n|  0|  1|    0|\n|  0|  2|    0|\n|  0|  3|    0|\n|  0|  4|    0|\n+---+---+-----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "//array of 784 ints (28*28)\n//converts the array of ints to a dataframe x,y,int, for scatter plot\ndef rowToTable(r:org.apache.spark.sql.Row)={\n   val t = new Array[(Int,Int,Int)](28*28)\n   for(x <- 0 to 27){\n        for(y <- 0 to 27){\n          val p=r.getInt(1+x*28+y)//+1 because id\n            t(x*28+y)=(x,y,p)\n        }        \n    }\n    sc.parallelize(t).toDF(\"x\",\"y\",\"color\")//convert to DF\n}\nval pixelsDF = rowToTable(testRecord) //still an array x,y,int\npixelsDF.show(5)", 
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 8, 
                    "data": {
                        "text/html": "\n         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/brunel.2.0.css\" charset=\"utf-8\">\n         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/sumoselect.css\" charset=\"utf-8\">\n         <style>  </style>\n         <div id=\"controlsId479db455-6fa4-4773-a817-26fd9a0abe7e\" class=\"brunel\"/>\n<svg id=\"visid5f752487-2c75-4077-b2a7-32b9a1670e9d\" width=\"800\" height=\"300\"></svg>\n\n<script>\nrequire.config({\n            waitSeconds: 60,\n            paths: {\n                'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n                'topojson' : '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n                'brunel' : 'https://brunelvis.org/js/brunel.2.0.min',\n                'brunelControls' : 'https://brunelvis.org/js/brunel.controls.2.0.min'\n            },\n\n            shim: {\n               'brunel' : {\n                    exports: 'BrunelD3',\n                    deps: ['d3', 'topojson'],\n                    init: function() {\n                       return {\n                         BrunelD3 : BrunelD3,\n                         BrunelData : BrunelData\n                      }\n                    }\n                },\n               'brunelControls' : {\n                    exports: 'BrunelEventHandlers',\n                    init: function() {\n                       return {\n                         BrunelEventHandlers: BrunelEventHandlers,\n                         BrunelJQueryControlFactory: BrunelJQueryControlFactory\n                      }\n                    }\n                }\n\n            }\n\n        });\n\n        require([\"d3\"], function(d3) {\n        require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n\n            function  BrunelVis(visId) {\n  \"use strict\"; // Strict Mode\n  var datasets = [],                               // Array of datasets for the original data\n      pre = function(d, i) { return d },           // Default pre-process does nothing\n      post = function(d, i) { return d },          // Default post-process does nothing\n      transitionTime = 200,                        // Transition time for animations\n      charts = [],                                 // The charts in the system\n      hasData = function(d) {return d && (d.row != null || hasData(d.data))}, // Filters to data items\n      vis = d3.select('#' + visId).attr('class', 'brunel');  // the SVG container\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 46, 39, 63),\n      elements = [];                               // Array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element')\n      .attr('class', 'overlay').style('cursor','move').style('fill','none').style('pointer-events','all');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .call(zoom)\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visid5f752487-2c75-4077-b2a7-32b9a1670e9d_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    var legends = chart.append('g').attr('class', 'legend')\n      .attr('transform','translate(' + (geom.chart_right-geom.chart_left - 3) + ',' + 0 + ')');\n    vis.append('clipPath').attr('id', 'clip_visid5f752487-2c75-4077-b2a7-32b9a1670e9d_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scaleLinear()\n      .domain([-5, 30.000003])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scaleLinear()\n      .domain([-5, 30.000003])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];          // Untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_visid5f752487-2c75-4077-b2a7-32b9a1670e9d_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_visid5f752487-2c75-4077-b2a7-32b9a1670e9d_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('x').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_visid5f752487-2c75-4077-b2a7-32b9a1670e9d_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_visid5f752487-2c75-4077-b2a7-32b9a1670e9d_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('y').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).tickSizeInner(3).tickPadding(3).tickSizeOuter(0)\n      .ticks(Math.min(10, Math.round(geom.inner_width / 37.5)));\n    var axis_left = d3.axisLeft(scale_y).tickSizeInner(3).tickPadding(3).tickSizeOuter(0);\n\n    function buildAxes(time) {\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        scale_x = t.rescaleX(base_scales[0]);\n        scale_y = t.rescaleY(base_scales[1]);\n        zoomNode.__zoom = t;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,           // data sets passed in and then transformed\n        element, data,                   // Brunel element information and brunel data\n        selection, merged;               // D3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0);\n        processed = post(processed, 0);\n        var f0 = processed.field('x'),\n          f1 = processed.field('y'),\n          f2 = processed.field('color'),\n          f3 = processed.field('#row'),\n          f4 = processed.field('#selection');\n        var keyFunc = function(d) { return f3.value(d) };\n        data = {\n          x:            function(d) { return f0.value(d.row) },\n          y:            function(d) { return f1.value(d.row) },\n          color:        function(d) { return f2.value(d.row) },\n          $row:         function(d) { return f3.value(d.row) },\n          $selection:   function(d) { return f4.value(d.row) },\n          x_f:          function(d) { return f0.valueFormatted(d.row) },\n          y_f:          function(d) { return f1.valueFormatted(d.row) },\n          color_f:      function(d) { return f2.valueFormatted(d.row) },\n          $row_f:       function(d) { return f3.valueFormatted(d.row) },\n          $selection_f: function(d) { return f4.valueFormatted(d.row) },\n          _split:       function(d) { return f2.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleSqrt()\n        .domain([0, 3.984375, 15.9375, 35.859375, 63.75, 99.609375, 143.4375, 195.23437, 255])\n        .interpolate(d3.interpolateHcl)\n        .range([ '#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1', '#f8efe8', '#fef0d9', \n          '#fdcc8a', '#fc8d59', '#e34a33']);\n      var color = function(d) { return scale_color(data.color(d)) };\n      BrunelD3.addLegend(legends, 'color', scale_color, [300, 250, 200, 150, 100, 50, 0]);\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = Math.abs( scale_x(scale_x.domain()[0] + 1.0) - scale_x.range()[0] );\n        var x = function(d) { return scale_x(data.x(d))};\n        var h = Math.abs( scale_y(scale_y.domain()[0] + 1.0) - scale_y.range()[0] );\n        var y = function(d) { return scale_y(data.y(d))};\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key});\n        var added = selection.enter().append('circle')\n          .attr('class', 'element point filled')\n          .style('pointer-events', 'none');\n\n        merged = selection.merge(added);\n        merged.filter(hasData).classed('selected', function(d) { return data.$selection(d) == '\u2713' });\n        BrunelD3.transition(merged, transitionMillis)\n          .attr('cx',function(d) { return scale_x(data.x(d))})\n          .attr('cy',function(d) { return scale_y(data.y(d))})\n          .attr('r',Math.min(Math.abs( scale_x(scale_x.domain()[0] + 1.0) - scale_x.range()[0] ), Math.abs( scale_y(scale_y.domain()[0] + 1.0) - scale_y.range()[0] )) / 2)\n          .filter(hasData)                         // following only performed for data items\n          .style('fill', color);\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); if (this.__label__) this.__label__.remove()\n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['x'],\n          y:            ['y'],\n          key:          ['#row'],\n          color:        ['color']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0; // No transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData)elements[0].makeData();\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   names: ['x', 'y', 'color'], \n   options: ['numeric', 'numeric', 'numeric'], \n   rows: [[0, 0, 0], [0, 1, 0], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0],\n  [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 0], [0, 12, 0], [0, 13, 0], [0, 14, 0], [0, 15, 0],\n  [0, 16, 0], [0, 17, 0], [0, 18, 0], [0, 19, 0], [0, 20, 0], [0, 21, 0], [0, 22, 0], [0, 23, 0],\n  [0, 24, 0], [0, 25, 0], [0, 26, 0], [0, 27, 0], [1, 0, 0], [1, 1, 0], [1, 2, 0], [1, 3, 0],\n  [1, 4, 0], [1, 5, 0], [1, 6, 0], [1, 7, 0], [1, 8, 0], [1, 9, 0], [1, 10, 0], [1, 11, 0],\n  [1, 12, 0], [1, 13, 0], [1, 14, 0], [1, 15, 0], [1, 16, 0], [1, 17, 0], [1, 18, 0], [1, 19, 0],\n  [1, 20, 0], [1, 21, 0], [1, 22, 0], [1, 23, 0], [1, 24, 0], [1, 25, 0], [1, 26, 0], [1, 27, 0],\n  [2, 0, 0], [2, 1, 0], [2, 2, 0], [2, 3, 0], [2, 4, 0], [2, 5, 0], [2, 6, 0], [2, 7, 0], [2, 8, 0],\n  [2, 9, 0], [2, 10, 0], [2, 11, 0], [2, 12, 0], [2, 13, 0], [2, 14, 0], [2, 15, 0], [2, 16, 0],\n  [2, 17, 0], [2, 18, 0], [2, 19, 0], [2, 20, 0], [2, 21, 0], [2, 22, 0], [2, 23, 0], [2, 24, 0],\n  [2, 25, 0], [2, 26, 0], [2, 27, 0], [3, 0, 0], [3, 1, 0], [3, 2, 0], [3, 3, 0], [3, 4, 0],\n  [3, 5, 0], [3, 6, 0], [3, 7, 0], [3, 8, 0], [3, 9, 0], [3, 10, 0], [3, 11, 0], [3, 12, 0],\n  [3, 13, 0], [3, 14, 0], [3, 15, 0], [3, 16, 0], [3, 17, 0], [3, 18, 0], [3, 19, 0], [3, 20, 0],\n  [3, 21, 0], [3, 22, 0], [3, 23, 0], [3, 24, 0], [3, 25, 0], [3, 26, 0], [3, 27, 0], [4, 0, 0],\n  [4, 1, 0], [4, 2, 0], [4, 3, 0], [4, 4, 0], [4, 5, 0], [4, 6, 0], [4, 7, 0], [4, 8, 0], [4, 9, 0],\n  [4, 10, 0], [4, 11, 0], [4, 12, 0], [4, 13, 0], [4, 14, 0], [4, 15, 0], [4, 16, 0], [4, 17, 0],\n  [4, 18, 0], [4, 19, 0], [4, 20, 0], [4, 21, 0], [4, 22, 0], [4, 23, 0], [4, 24, 0], [4, 25, 0],\n  [4, 26, 0], [4, 27, 0], [5, 0, 0], [5, 1, 0], [5, 2, 0], [5, 3, 0], [5, 4, 0], [5, 5, 0],\n  [5, 6, 0], [5, 7, 0], [5, 8, 0], [5, 9, 0], [5, 10, 0], [5, 11, 0], [5, 12, 0], [5, 13, 3],\n  [5, 14, 18], [5, 15, 18], [5, 16, 18], [5, 17, 126], [5, 18, 136], [5, 19, 175], [5, 20, 26],\n  [5, 21, 166], [5, 22, 255], [5, 23, 247], [5, 24, 127], [5, 25, 0], [5, 26, 0], [5, 27, 0],\n  [6, 0, 0], [6, 1, 0], [6, 2, 0], [6, 3, 0], [6, 4, 0], [6, 5, 0], [6, 6, 0], [6, 7, 0], [6, 8, 0],\n  [6, 9, 30], [6, 10, 36], [6, 11, 94], [6, 12, 154], [6, 13, 170], [6, 14, 253], [6, 15, 253],\n  [6, 16, 253], [6, 17, 253], [6, 18, 253], [6, 19, 225], [6, 20, 172], [6, 21, 253], [6, 22, 242],\n  [6, 23, 195], [6, 24, 64], [6, 25, 0], [6, 26, 0], [6, 27, 0], [7, 0, 0], [7, 1, 0], [7, 2, 0],\n  [7, 3, 0], [7, 4, 0], [7, 5, 0], [7, 6, 0], [7, 7, 0], [7, 8, 49], [7, 9, 238], [7, 10, 253],\n  [7, 11, 253], [7, 12, 253], [7, 13, 253], [7, 14, 253], [7, 15, 253], [7, 16, 253], [7, 17, 253],\n  [7, 18, 251], [7, 19, 93], [7, 20, 82], [7, 21, 82], [7, 22, 56], [7, 23, 39], [7, 24, 0],\n  [7, 25, 0], [7, 26, 0], [7, 27, 0], [8, 0, 0], [8, 1, 0], [8, 2, 0], [8, 3, 0], [8, 4, 0],\n  [8, 5, 0], [8, 6, 0], [8, 7, 0], [8, 8, 18], [8, 9, 219], [8, 10, 253], [8, 11, 253], [8, 12, 253],\n  [8, 13, 253], [8, 14, 253], [8, 15, 198], [8, 16, 182], [8, 17, 247], [8, 18, 241], [8, 19, 0],\n  [8, 20, 0], [8, 21, 0], [8, 22, 0], [8, 23, 0], [8, 24, 0], [8, 25, 0], [8, 26, 0], [8, 27, 0],\n  [9, 0, 0], [9, 1, 0], [9, 2, 0], [9, 3, 0], [9, 4, 0], [9, 5, 0], [9, 6, 0], [9, 7, 0], [9, 8, 0],\n  [9, 9, 80], [9, 10, 156], [9, 11, 107], [9, 12, 253], [9, 13, 253], [9, 14, 205], [9, 15, 11],\n  [9, 16, 0], [9, 17, 43], [9, 18, 154], [9, 19, 0], [9, 20, 0], [9, 21, 0], [9, 22, 0], [9, 23, 0],\n  [9, 24, 0], [9, 25, 0], [9, 26, 0], [9, 27, 0], [10, 0, 0], [10, 1, 0], [10, 2, 0], [10, 3, 0],\n  [10, 4, 0], [10, 5, 0], [10, 6, 0], [10, 7, 0], [10, 8, 0], [10, 9, 0], [10, 10, 14], [10, 11, 1],\n  [10, 12, 154], [10, 13, 253], [10, 14, 90], [10, 15, 0], [10, 16, 0], [10, 17, 0], [10, 18, 0],\n  [10, 19, 0], [10, 20, 0], [10, 21, 0], [10, 22, 0], [10, 23, 0], [10, 24, 0], [10, 25, 0],\n  [10, 26, 0], [10, 27, 0], [11, 0, 0], [11, 1, 0], [11, 2, 0], [11, 3, 0], [11, 4, 0], [11, 5, 0],\n  [11, 6, 0], [11, 7, 0], [11, 8, 0], [11, 9, 0], [11, 10, 0], [11, 11, 0], [11, 12, 139],\n  [11, 13, 253], [11, 14, 190], [11, 15, 2], [11, 16, 0], [11, 17, 0], [11, 18, 0], [11, 19, 0],\n  [11, 20, 0], [11, 21, 0], [11, 22, 0], [11, 23, 0], [11, 24, 0], [11, 25, 0], [11, 26, 0],\n  [11, 27, 0], [12, 0, 0], [12, 1, 0], [12, 2, 0], [12, 3, 0], [12, 4, 0], [12, 5, 0], [12, 6, 0],\n  [12, 7, 0], [12, 8, 0], [12, 9, 0], [12, 10, 0], [12, 11, 0], [12, 12, 11], [12, 13, 190],\n  [12, 14, 253], [12, 15, 70], [12, 16, 0], [12, 17, 0], [12, 18, 0], [12, 19, 0], [12, 20, 0],\n  [12, 21, 0], [12, 22, 0], [12, 23, 0], [12, 24, 0], [12, 25, 0], [12, 26, 0], [12, 27, 0],\n  [13, 0, 0], [13, 1, 0], [13, 2, 0], [13, 3, 0], [13, 4, 0], [13, 5, 0], [13, 6, 0], [13, 7, 0],\n  [13, 8, 0], [13, 9, 0], [13, 10, 0], [13, 11, 0], [13, 12, 0], [13, 13, 35], [13, 14, 241],\n  [13, 15, 225], [13, 16, 160], [13, 17, 108], [13, 18, 1], [13, 19, 0], [13, 20, 0], [13, 21, 0],\n  [13, 22, 0], [13, 23, 0], [13, 24, 0], [13, 25, 0], [13, 26, 0], [13, 27, 0], [14, 0, 0],\n  [14, 1, 0], [14, 2, 0], [14, 3, 0], [14, 4, 0], [14, 5, 0], [14, 6, 0], [14, 7, 0], [14, 8, 0],\n  [14, 9, 0], [14, 10, 0], [14, 11, 0], [14, 12, 0], [14, 13, 0], [14, 14, 81], [14, 15, 240],\n  [14, 16, 253], [14, 17, 253], [14, 18, 119], [14, 19, 25], [14, 20, 0], [14, 21, 0], [14, 22, 0],\n  [14, 23, 0], [14, 24, 0], [14, 25, 0], [14, 26, 0], [14, 27, 0], [15, 0, 0], [15, 1, 0],\n  [15, 2, 0], [15, 3, 0], [15, 4, 0], [15, 5, 0], [15, 6, 0], [15, 7, 0], [15, 8, 0], [15, 9, 0],\n  [15, 10, 0], [15, 11, 0], [15, 12, 0], [15, 13, 0], [15, 14, 0], [15, 15, 45], [15, 16, 186],\n  [15, 17, 253], [15, 18, 253], [15, 19, 150], [15, 20, 27], [15, 21, 0], [15, 22, 0], [15, 23, 0],\n  [15, 24, 0], [15, 25, 0], [15, 26, 0], [15, 27, 0], [16, 0, 0], [16, 1, 0], [16, 2, 0], [16, 3, 0],\n  [16, 4, 0], [16, 5, 0], [16, 6, 0], [16, 7, 0], [16, 8, 0], [16, 9, 0], [16, 10, 0], [16, 11, 0],\n  [16, 12, 0], [16, 13, 0], [16, 14, 0], [16, 15, 0], [16, 16, 16], [16, 17, 93], [16, 18, 252],\n  [16, 19, 253], [16, 20, 187], [16, 21, 0], [16, 22, 0], [16, 23, 0], [16, 24, 0], [16, 25, 0],\n  [16, 26, 0], [16, 27, 0], [17, 0, 0], [17, 1, 0], [17, 2, 0], [17, 3, 0], [17, 4, 0], [17, 5, 0],\n  [17, 6, 0], [17, 7, 0], [17, 8, 0], [17, 9, 0], [17, 10, 0], [17, 11, 0], [17, 12, 0], [17, 13, 0],\n  [17, 14, 0], [17, 15, 0], [17, 16, 0], [17, 17, 0], [17, 18, 249], [17, 19, 253], [17, 20, 249],\n  [17, 21, 64], [17, 22, 0], [17, 23, 0], [17, 24, 0], [17, 25, 0], [17, 26, 0], [17, 27, 0],\n  [18, 0, 0], [18, 1, 0], [18, 2, 0], [18, 3, 0], [18, 4, 0], [18, 5, 0], [18, 6, 0], [18, 7, 0],\n  [18, 8, 0], [18, 9, 0], [18, 10, 0], [18, 11, 0], [18, 12, 0], [18, 13, 0], [18, 14, 0],\n  [18, 15, 46], [18, 16, 130], [18, 17, 183], [18, 18, 253], [18, 19, 253], [18, 20, 207],\n  [18, 21, 2], [18, 22, 0], [18, 23, 0], [18, 24, 0], [18, 25, 0], [18, 26, 0], [18, 27, 0],\n  [19, 0, 0], [19, 1, 0], [19, 2, 0], [19, 3, 0], [19, 4, 0], [19, 5, 0], [19, 6, 0], [19, 7, 0],\n  [19, 8, 0], [19, 9, 0], [19, 10, 0], [19, 11, 0], [19, 12, 0], [19, 13, 39], [19, 14, 148],\n  [19, 15, 229], [19, 16, 253], [19, 17, 253], [19, 18, 253], [19, 19, 250], [19, 20, 182],\n  [19, 21, 0], [19, 22, 0], [19, 23, 0], [19, 24, 0], [19, 25, 0], [19, 26, 0], [19, 27, 0],\n  [20, 0, 0], [20, 1, 0], [20, 2, 0], [20, 3, 0], [20, 4, 0], [20, 5, 0], [20, 6, 0], [20, 7, 0],\n  [20, 8, 0], [20, 9, 0], [20, 10, 0], [20, 11, 24], [20, 12, 114], [20, 13, 221], [20, 14, 253],\n  [20, 15, 253], [20, 16, 253], [20, 17, 253], [20, 18, 201], [20, 19, 78], [20, 20, 0], [20, 21, 0],\n  [20, 22, 0], [20, 23, 0], [20, 24, 0], [20, 25, 0], [20, 26, 0], [20, 27, 0], [21, 0, 0],\n  [21, 1, 0], [21, 2, 0], [21, 3, 0], [21, 4, 0], [21, 5, 0], [21, 6, 0], [21, 7, 0], [21, 8, 0],\n  [21, 9, 23], [21, 10, 66], [21, 11, 213], [21, 12, 253], [21, 13, 253], [21, 14, 253],\n  [21, 15, 253], [21, 16, 198], [21, 17, 81], [21, 18, 2], [21, 19, 0], [21, 20, 0], [21, 21, 0],\n  [21, 22, 0], [21, 23, 0], [21, 24, 0], [21, 25, 0], [21, 26, 0], [21, 27, 0], [22, 0, 0],\n  [22, 1, 0], [22, 2, 0], [22, 3, 0], [22, 4, 0], [22, 5, 0], [22, 6, 0], [22, 7, 18], [22, 8, 171],\n  [22, 9, 219], [22, 10, 253], [22, 11, 253], [22, 12, 253], [22, 13, 253], [22, 14, 195],\n  [22, 15, 80], [22, 16, 9], [22, 17, 0], [22, 18, 0], [22, 19, 0], [22, 20, 0], [22, 21, 0],\n  [22, 22, 0], [22, 23, 0], [22, 24, 0], [22, 25, 0], [22, 26, 0], [22, 27, 0], [23, 0, 0],\n  [23, 1, 0], [23, 2, 0], [23, 3, 0], [23, 4, 0], [23, 5, 55], [23, 6, 172], [23, 7, 226],\n  [23, 8, 253], [23, 9, 253], [23, 10, 253], [23, 11, 253], [23, 12, 244], [23, 13, 133],\n  [23, 14, 11], [23, 15, 0], [23, 16, 0], [23, 17, 0], [23, 18, 0], [23, 19, 0], [23, 20, 0],\n  [23, 21, 0], [23, 22, 0], [23, 23, 0], [23, 24, 0], [23, 25, 0], [23, 26, 0], [23, 27, 0],\n  [24, 0, 0], [24, 1, 0], [24, 2, 0], [24, 3, 0], [24, 4, 0], [24, 5, 136], [24, 6, 253],\n  [24, 7, 253], [24, 8, 253], [24, 9, 212], [24, 10, 135], [24, 11, 132], [24, 12, 16], [24, 13, 0],\n  [24, 14, 0], [24, 15, 0], [24, 16, 0], [24, 17, 0], [24, 18, 0], [24, 19, 0], [24, 20, 0],\n  [24, 21, 0], [24, 22, 0], [24, 23, 0], [24, 24, 0], [24, 25, 0], [24, 26, 0], [24, 27, 0],\n  [25, 0, 0], [25, 1, 0], [25, 2, 0], [25, 3, 0], [25, 4, 0], [25, 5, 0], [25, 6, 0], [25, 7, 0],\n  [25, 8, 0], [25, 9, 0], [25, 10, 0], [25, 11, 0], [25, 12, 0], [25, 13, 0], [25, 14, 0],\n  [25, 15, 0], [25, 16, 0], [25, 17, 0], [25, 18, 0], [25, 19, 0], [25, 20, 0], [25, 21, 0],\n  [25, 22, 0], [25, 23, 0], [25, 24, 0], [25, 25, 0], [25, 26, 0], [25, 27, 0], [26, 0, 0],\n  [26, 1, 0], [26, 2, 0], [26, 3, 0], [26, 4, 0], [26, 5, 0], [26, 6, 0], [26, 7, 0], [26, 8, 0],\n  [26, 9, 0], [26, 10, 0], [26, 11, 0], [26, 12, 0], [26, 13, 0], [26, 14, 0], [26, 15, 0],\n  [26, 16, 0], [26, 17, 0], [26, 18, 0], [26, 19, 0], [26, 20, 0], [26, 21, 0], [26, 22, 0],\n  [26, 23, 0], [26, 24, 0], [26, 25, 0], [26, 26, 0], [26, 27, 0], [27, 0, 0], [27, 1, 0],\n  [27, 2, 0], [27, 3, 0], [27, 4, 0], [27, 5, 0], [27, 6, 0], [27, 7, 0], [27, 8, 0], [27, 9, 0],\n  [27, 10, 0], [27, 11, 0], [27, 12, 0], [27, 13, 0], [27, 14, 0], [27, 15, 0], [27, 16, 0],\n  [27, 17, 0], [27, 18, 0], [27, 19, 0], [27, 20, 0], [27, 21, 0], [27, 22, 0], [27, 23, 0],\n  [27, 24, 0], [27, 25, 0], [27, 26, 0], [27, 27, 0]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v = new BrunelVis('visid5f752487-2c75-4077-b2a7-32b9a1670e9d');\nv.build(table1);\n\n            \"\"\n        });\n        });\n        </script>"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "%%brunel data('pixelsDF') x(x) y(y) color(color) :: width=800, height=300", 
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Load the labels for the training set\ntrain-labels.idx1-ubyte", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "var credentials_train_labels = scala.collection.mutable.HashMap[String, String](\n  \"auth_url\"->\"https://identity.open.softlayer.com\",\n  \"project\"->\"object_storage_xxxx\",\n  \"project_id\"->\"xxxx\",\n  \"region\"->\"dallas\",\n  \"user_id\"->\"xxxx\",\n  \"domain_id\"->\"xxx\",\n  \"domain_name\"->\"xxxx\",\n  \"username\"->\"member_xxxx\",\n  \"password\"->\"\"\"xxxxx\"\"\",\n  \"container\"->\"FirstProject\",\n  \"tenantId\"->\"undefined\",\n  \"filename\"->\"train-labels.idx1-ubyte\"\n)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "swift://FirstProject.spark/train-labels.idx1-ubyte"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "set_hadoop_config_with_credentials(credentials_train_labels)\nval sFilePathTrainLabels = build_swift_url(credentials_train_labels)\nsFilePathTrainLabels", 
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+-----+\n| id|label|\n+---+-----+\n|0.0|  5.0|\n|1.0|  0.0|\n|2.0|  4.0|\n|3.0|  1.0|\n|4.0|  9.0|\n+---+-----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "//Read label file - binary\n//header is smaller for label files : 8\nval trainLabelsDF=loadidxFile(sFilePathTrainLabels,\"l\",8,1,trainSetSize).withColumnRenamed(\"l0\", \"label\")// 60000\ntrainLabelsDF.count()\ntrainLabelsDF.show(5)", 
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Prepare the training dataset by joining images and labels\ntrainImagesDF\ntrainLabelsDF", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Combine the two dataframes", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "def prepareMLPdata(imagesDF:org.apache.spark.sql.DataFrame,labelsDF:org.apache.spark.sql.DataFrame)={\n    val joinedDF = labelsDF.join(imagesDF,Seq(\"id\"))\n    import org.apache.spark.ml.feature.VectorAssembler\n    val aCols = (0 until 784).toArray.map(i => \"p\"+i)\n    val assembler = new VectorAssembler().setInputCols(aCols).setOutputCol(\"features\")\n    assembler.transform(joinedDF).select(\"label\",\"features\")\n}", 
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  3.0|(784,[151,152,153...|\n|  0.0|(784,[127,128,129...|\n|  4.0|(784,[134,135,161...|\n|  1.0|(784,[124,125,126...|\n|  1.0|(784,[158,159,160...|\n+-----+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "val trainData=prepareMLPdata(trainImagesDF,trainLabelsDF)\ntrainData.show(5)", 
            "execution_count": 57, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Build the MLP", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n// specify layers for the neural network:\n// input layer of size 28*28 (features), \n// intermediate of size 300\n// and output of size 10 (classes)\nval layers = Array[Int](784, 300, 10)\n// create the trainer and set its parameters\nval trainer = new MultilayerPerceptronClassifier().\n  setLayers(layers).\n  setBlockSize(128).\n  setSeed(1234L).\n  setMaxIter(100)", 
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Train the model", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "// train the model\nval model = trainer.fit(trainData)", 
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Load test data\nt10k-images.idx3-ubyte to testImagesDF\nt10k-labels.idx1-ubyte to testLabelsDF", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "var credentials_test_images = scala.collection.mutable.HashMap[String, String](\n  \"auth_url\"->\"https://identity.open.softlayer.com\",\n  \"project\"->\"object_storage_cxxxxx\",\n  \"project_id\"->\"xxxxx\",\n  \"region\"->\"dallas\",\n  \"user_id\"->\"xxxxx\",\n  \"domain_id\"->\"xxxxx\",\n  \"domain_name\"->\"xxxxx\",\n  \"username\"->\"member_xxxxx\",\n  \"password\"->\"\"\"xxxxx\"\"\",\n  \"container\"->\"FirstProject\",\n  \"tenantId\"->\"undefined\",\n  \"filename\"->\"t10k-images.idx3-ubyte\"\n)\n\nvar credentials_test_labels = scala.collection.mutable.HashMap[String, String](\n  \"auth_url\"->\"https://identity.open.softlayer.com\",\n  \"project\"->\"object_storage_xxxxx\",\n  \"project_id\"->\"xxxxx\",\n  \"region\"->\"dallas\",\n  \"user_id\"->\"xxxxx\",\n  \"domain_id\"->\"xxxxx\",\n  \"domain_name\"->\"xxxxx\",\n  \"username\"->\"member_xxxxx\",\n  \"password\"->\"\"\"xxxxx\"\"\",\n  \"container\"->\"FirstProject\",\n  \"tenantId\"->\"undefined\",\n  \"filename\"->\"t10k-labels.idx1-ubyte\"\n)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 79, 
                    "data": {
                        "text/plain": "Name: Compile Error\nMessage: <console>:34: error: not found: value loadidxFile\n         val testImagesDF = loadidxFile(sFilePathTestImages,\"p\",16,784,testSetSize)\n                            ^\nStackTrace: "
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "set_hadoop_config_with_credentials(credentials_test_images)\nval sFilePathTestImages = build_swift_url(credentials_test_images)\nval testImagesDF = loadidxFile(sFilePathTestImages,\"p\",16,784,testSetSize)\n\nset_hadoop_config_with_credentials(credentials_test_labels)\nval sFilePathTestLabels = build_swift_url(credentials_test_labels)\nval testLabelsDF = loadidxFile(sFilePathTestLabels,\"l\",8,1,testSetSize).withColumnRenamed(\"l0\", \"label\")\nval testData=prepareMLPdata(testImagesDF,testLabelsDF)\ntestData.count()", 
            "execution_count": 79, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Test the model", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Accuracy: 0.3964\n"
                }
            ], 
            "source": "// compute accuracy on the test set\nval result = model.transform(testData)\nresult.describe()\nval predictionAndLabels = result.select(\"prediction\", \"label\")\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval evaluator = new MulticlassClassificationEvaluator().setMetricName(\"precision\")//accuracy\")\nprintln(\"Accuracy: \" + evaluator.evaluate(predictionAndLabels))", 
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# Ex 3 optimize", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Loader\nLet's rewrite the file loader to build the training dataset in one go - to avoid joining the label and image datasets", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "//V1, read the file to an array of array of bytes, then convert to a DataFrame using a dynamic schema, then creates a vector via vectorAssembler\n//Not optimised at all because we go 3 times trough the data\ndef loadImageAndLabels_v1(sImageFilePath:String,sLabelFilePath:String,iRecordNum:Int)={\n    val imageStream = sc.binaryFiles(sImageFilePath, 1).first._2.open()\n    val labelStream = sc.binaryFiles(sLabelFilePath, 1).first._2.open()    \n    imageStream.skipBytes(16)\n    labelStream.skipBytes(8)\n    val records = new Array[Array[Double]](iRecordNum)\n    val iImageLength = 784//pixels\n    for(r <- 0 to iRecordNum-1){\n      val iRecordLength = iImageLength+1//label+pixels\n      import org.apache.spark.ml.feature.VectorAssembler\n      val record = new Array[Double](iRecordLength)\n      record(0) = labelStream.readUnsignedByte().toDouble //label  \n      for(p <- 1 to iImageLength){\n          record(p)=imageStream.readUnsignedByte().toDouble\n      }        \n      records(r)=record     \n    }\n    //we now have an array of array, lets transform to a dataframe\n    val arrayRDD = sc.parallelize(records)\n    //  Generate the schema based on the number of fields\n    import org.apache.spark.sql.types._\n    val fields = StructField(\"label\", DoubleType, nullable = false)::(0 until iImageLength).toList.map(i => StructField(\"p\"+i, DoubleType, nullable = true))\n    val schema = StructType(fields)\n    import org.apache.spark.sql._\n    val dataDF = sqlContext.createDataFrame(arrayRDD.map(s => Row.fromSeq(s.toSeq)),schema)\n    // Now assemble the pixels columns to form the features vector. Note : I was unable to find how to build a vector from scratch.\n    import org.apache.spark.ml.feature.VectorAssembler\n    val aCols = (0 until iImageLength).toArray.map(i => \"p\"+i)\n    val assembler = new VectorAssembler().setInputCols(aCols).setOutputCol(\"features\")\n    assembler.transform(dataDF).select(\"label\",\"features\")\n}  ", 
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "//V2, we build the vector directly after reading each record, better, but it's still sequential\ndef loadImageAndLabels_V2(sImageFilePath:String,sLabelFilePath:String,iRecordNum:Int)={\n    val imageStream = sc.binaryFiles(sImageFilePath, 1).first._2.open()\n    val labelStream = sc.binaryFiles(sLabelFilePath, 1).first._2.open()    \n    imageStream.skipBytes(16)\n    labelStream.skipBytes(8)\n    import org.apache.spark.mllib.linalg.DenseVector\n    val records = new Array[(Double,DenseVector)](iRecordNum)\n    val iImageLength = 784//pixels\n    for(r <- 0 to iRecordNum-1){\n      val dLabel = labelStream.readUnsignedByte().toDouble //label\n      val pixelArray = new Array[Double](iImageLength)  \n      for(p <- 0 to iImageLength-1){\n          pixelArray(p)=imageStream.readUnsignedByte().toDouble\n      }        \n      val fVector = new DenseVector(pixelArray)\n      records(r)=(dLabel,fVector)     \n    }\n    //we now have an array, lets transform to a dataframe\n    sc.parallelize(records).toDF(\"label\",\"features\")\n  }", 
            "execution_count": 88, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "//V3, Now we read the array as a big byte array, and do the conversion later in the RDD so it's parallel\ndef loadImageAndLabels(sImageFilePath:String,sLabelFilePath:String,iRecordNum:Int)={\n    val imageStream = sc.binaryFiles(sImageFilePath, 1).first._2.open()\n    val labelStream = sc.binaryFiles(sLabelFilePath, 1).first._2.open()    \n    imageStream.skipBytes(16)\n    labelStream.skipBytes(8)\n    import org.apache.spark.mllib.linalg.DenseVector\n    val records = new Array[(Double,Array[Byte])](iRecordNum)\n    val iImageLength = 784//pixels\n    var iFrom=0\n    for(r <- 0 to iRecordNum-1){\n      val dLabel = labelStream.readUnsignedByte().toDouble //label\n      val pixelArray = new Array[Byte](iImageLength)//we read the array as a big byte array, the conversion will be done later in the RDD\n      //imageStream.read(pixelArray,iFrom,iImageLength)\n      imageStream.read(pixelArray)\n      iFrom=iFrom+iImageLength\n      records(r)=(dLabel,pixelArray)     \n    }\n    //we need to convert byte array to vector\n    def byteArrayToVector(byteArray:Array[Byte]):DenseVector={\n       val pixelArray = new Array[Double](byteArray.size) \n       for(p <- 0 to pixelArray.size-1){\n            pixelArray(p)=(byteArray(p) & 0xff).toDouble            \n       }\n       new DenseVector(pixelArray)\n    }   \n    //we now have an array, lets transform to a dataframe\n    sc.parallelize(records).map(p=>(p._1,byteArrayToVector(p._2))).toDF(\"label\",\"features\")\n   } ", 
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "val trainData = loadImageAndLabels(build_swift_url(credentials_train_images),build_swift_url(credentials_train_labels),trainSetSize).cache()", 
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "val testData = loadImageAndLabels(build_swift_url(credentials_test_images),build_swift_url(credentials_test_labels),testSetSize).cache()", 
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  5.0|[0.0,0.0,0.0,0.0,...|\n|  0.0|[0.0,0.0,0.0,0.0,...|\n|  4.0|[0.0,0.0,0.0,0.0,...|\n|  1.0|[0.0,0.0,0.0,0.0,...|\n|  9.0|[0.0,0.0,0.0,0.0,...|\n+-----+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "trainData.show(5)", 
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Preview\nLet's rewrite the preview function to take the train data and display the image and the label", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "def previewRecord(dataDF:org.apache.spark.sql.DataFrame,iRowNum:Int)={\n   val aPixelList = new Array[(Int,Int,Double)](28*28)\n   val rRow = dataDF.rdd.take(iRowNum).drop(iRowNum-1)(0)//TODO this is not optimal\n   val dLabel = rRow.get(0) match {//Double\n    case d: Double => d\n    case _ => throw new ClassCastException\n   }   \n   val vPixelVector  = rRow.get(1) match {\n    case dv: org.apache.spark.mllib.linalg.SparseVector => dv.toDense\n    case sv: org.apache.spark.mllib.linalg.DenseVector => sv\n    case _ => throw new ClassCastException\n   }\n   for(x <- 0 to 27){\n        for(y <- 0 to 27){\n          val idx = y*28+x\n          val pixel = vPixelVector.values(idx)//+1 because id\n          aPixelList(idx)=(x,28-y,pixel)\n        }        \n    }\n    val pixelsDF = sc.parallelize(aPixelList).toDF(\"x\",\"y\",\"color\")//convert to DF\n    println(dLabel)\n    pixelsDF\n}", 
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "5.0\n"
                }
            ], 
            "source": "val pixelsDF = previewRecord(trainData,12)", 
            "execution_count": 36, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 37, 
                    "data": {
                        "text/html": "\n         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/brunel.2.0.css\" charset=\"utf-8\">\n         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/sumoselect.css\" charset=\"utf-8\">\n         <style>  </style>\n         <div id=\"controlsIdcebdde27-a580-4156-a057-cda252e213ae\" class=\"brunel\"/>\n<svg id=\"visidebf5877f-a389-405b-ab2a-4cb525d71071\" width=\"400\" height=\"400\"></svg>\n\n<script>\nrequire.config({\n            waitSeconds: 60,\n            paths: {\n                'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n                'topojson' : '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n                'brunel' : 'https://brunelvis.org/js/brunel.2.0.min',\n                'brunelControls' : 'https://brunelvis.org/js/brunel.controls.2.0.min'\n            },\n\n            shim: {\n               'brunel' : {\n                    exports: 'BrunelD3',\n                    deps: ['d3', 'topojson'],\n                    init: function() {\n                       return {\n                         BrunelD3 : BrunelD3,\n                         BrunelData : BrunelData\n                      }\n                    }\n                },\n               'brunelControls' : {\n                    exports: 'BrunelEventHandlers',\n                    init: function() {\n                       return {\n                         BrunelEventHandlers: BrunelEventHandlers,\n                         BrunelJQueryControlFactory: BrunelJQueryControlFactory\n                      }\n                    }\n                }\n\n            }\n\n        });\n\n        require([\"d3\"], function(d3) {\n        require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n\n            function  BrunelVis(visId) {\n  \"use strict\"; // Strict Mode\n  var datasets = [],                               // Array of datasets for the original data\n      pre = function(d, i) { return d },           // Default pre-process does nothing\n      post = function(d, i) { return d },          // Default post-process does nothing\n      transitionTime = 200,                        // Transition time for animations\n      charts = [],                                 // The charts in the system\n      hasData = function(d) {return d && (d.row != null || hasData(d.data))}, // Filters to data items\n      vis = d3.select('#' + visId).attr('class', 'brunel');  // the SVG container\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 46, 39, 63),\n      elements = [];                               // Array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element')\n      .attr('class', 'overlay').style('cursor','move').style('fill','none').style('pointer-events','all');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .call(zoom)\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visidebf5877f-a389-405b-ab2a-4cb525d71071_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    var legends = chart.append('g').attr('class', 'legend')\n      .attr('transform','translate(' + (geom.chart_right-geom.chart_left - 3) + ',' + 0 + ')');\n    vis.append('clipPath').attr('id', 'clip_visidebf5877f-a389-405b-ab2a-4cb525d71071_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scaleLinear()\n      .domain([-5, 30.000003])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scaleLinear()\n      .domain([0, 30.000003])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];          // Untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_visidebf5877f-a389-405b-ab2a-4cb525d71071_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_visidebf5877f-a389-405b-ab2a-4cb525d71071_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('x').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_visidebf5877f-a389-405b-ab2a-4cb525d71071_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_visidebf5877f-a389-405b-ab2a-4cb525d71071_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('y').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).tickSizeInner(3).tickPadding(3).tickSizeOuter(0)\n      .ticks(7);\n    var axis_left = d3.axisLeft(scale_y).tickSizeInner(3).tickPadding(3).tickSizeOuter(0);\n\n    function buildAxes(time) {\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        scale_x = t.rescaleX(base_scales[0]);\n        scale_y = t.rescaleY(base_scales[1]);\n        zoomNode.__zoom = t;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,           // data sets passed in and then transformed\n        element, data,                   // Brunel element information and brunel data\n        selection, merged;               // D3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0);\n        processed = post(processed, 0);\n        var f0 = processed.field('x'),\n          f1 = processed.field('y'),\n          f2 = processed.field('color'),\n          f3 = processed.field('#row'),\n          f4 = processed.field('#selection');\n        var keyFunc = function(d) { return f3.value(d) };\n        data = {\n          x:            function(d) { return f0.value(d.row) },\n          y:            function(d) { return f1.value(d.row) },\n          color:        function(d) { return f2.value(d.row) },\n          $row:         function(d) { return f3.value(d.row) },\n          $selection:   function(d) { return f4.value(d.row) },\n          x_f:          function(d) { return f0.valueFormatted(d.row) },\n          y_f:          function(d) { return f1.valueFormatted(d.row) },\n          color_f:      function(d) { return f2.valueFormatted(d.row) },\n          $row_f:       function(d) { return f3.valueFormatted(d.row) },\n          $selection_f: function(d) { return f4.valueFormatted(d.row) },\n          _split:       function(d) { return f2.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleSqrt()\n        .domain([0, 3.984375, 15.9375, 35.859375, 63.75, 99.609375, 143.4375, 195.23437, 255])\n        .interpolate(d3.interpolateHcl)\n        .range([ '#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1', '#f8efe8', '#fef0d9', \n          '#fdcc8a', '#fc8d59', '#e34a33']);\n      var color = function(d) { return scale_color(data.color(d)) };\n      BrunelD3.addLegend(legends, 'color', scale_color, [300, 250, 200, 150, 100, 50, 0]);\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = Math.abs( scale_x(scale_x.domain()[0] + 1.0) - scale_x.range()[0] );\n        var x = function(d) { return scale_x(data.x(d))};\n        var h = Math.abs( scale_y(scale_y.domain()[0] + 1.0) - scale_y.range()[0] );\n        var y = function(d) { return scale_y(data.y(d))};\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key});\n        var added = selection.enter().append('circle')\n          .attr('class', 'element point filled')\n          .style('pointer-events', 'none');\n\n        merged = selection.merge(added);\n        merged.filter(hasData).classed('selected', function(d) { return data.$selection(d) == '\u2713' });\n        BrunelD3.transition(merged, transitionMillis)\n          .attr('cx',function(d) { return scale_x(data.x(d))})\n          .attr('cy',function(d) { return scale_y(data.y(d))})\n          .attr('r',Math.min(Math.abs( scale_x(scale_x.domain()[0] + 1.0) - scale_x.range()[0] ), Math.abs( scale_y(scale_y.domain()[0] + 1.0) - scale_y.range()[0] )) / 2)\n          .filter(hasData)                         // following only performed for data items\n          .style('fill', color);\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); if (this.__label__) this.__label__.remove()\n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['x'],\n          y:            ['y'],\n          key:          ['#row'],\n          color:        ['color']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0; // No transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData)elements[0].makeData();\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   names: ['x', 'y', 'color'], \n   options: ['numeric', 'numeric', 'numeric'], \n   rows: [[0, 28, 0], [1, 28, 0], [2, 28, 0], [3, 28, 0], [4, 28, 0], [5, 28, 0], [6, 28, 0],\n  [7, 28, 0], [8, 28, 0], [9, 28, 0], [10, 28, 0], [11, 28, 0], [12, 28, 0], [13, 28, 0],\n  [14, 28, 0], [15, 28, 0], [16, 28, 0], [17, 28, 0], [18, 28, 0], [19, 28, 0], [20, 28, 0],\n  [21, 28, 0], [22, 28, 0], [23, 28, 0], [24, 28, 0], [25, 28, 0], [26, 28, 0], [27, 28, 0],\n  [0, 27, 0], [1, 27, 0], [2, 27, 0], [3, 27, 0], [4, 27, 0], [5, 27, 0], [6, 27, 0], [7, 27, 0],\n  [8, 27, 0], [9, 27, 0], [10, 27, 0], [11, 27, 0], [12, 27, 0], [13, 27, 0], [14, 27, 0],\n  [15, 27, 0], [16, 27, 0], [17, 27, 0], [18, 27, 0], [19, 27, 0], [20, 27, 0], [21, 27, 0],\n  [22, 27, 0], [23, 27, 0], [24, 27, 0], [25, 27, 0], [26, 27, 0], [27, 27, 0], [0, 26, 0],\n  [1, 26, 0], [2, 26, 0], [3, 26, 0], [4, 26, 0], [5, 26, 0], [6, 26, 0], [7, 26, 0], [8, 26, 0],\n  [9, 26, 0], [10, 26, 0], [11, 26, 0], [12, 26, 0], [13, 26, 0], [14, 26, 0], [15, 26, 0],\n  [16, 26, 0], [17, 26, 0], [18, 26, 0], [19, 26, 0], [20, 26, 0], [21, 26, 0], [22, 26, 0],\n  [23, 26, 0], [24, 26, 0], [25, 26, 0], [26, 26, 0], [27, 26, 0], [0, 25, 0], [1, 25, 0],\n  [2, 25, 0], [3, 25, 0], [4, 25, 0], [5, 25, 0], [6, 25, 0], [7, 25, 0], [8, 25, 0], [9, 25, 0],\n  [10, 25, 0], [11, 25, 0], [12, 25, 0], [13, 25, 0], [14, 25, 0], [15, 25, 0], [16, 25, 0],\n  [17, 25, 0], [18, 25, 0], [19, 25, 0], [20, 25, 0], [21, 25, 0], [22, 25, 0], [23, 25, 0],\n  [24, 25, 0], [25, 25, 0], [26, 25, 0], [27, 25, 0], [0, 24, 0], [1, 24, 0], [2, 24, 0], [3, 24, 0],\n  [4, 24, 0], [5, 24, 0], [6, 24, 0], [7, 24, 0], [8, 24, 0], [9, 24, 0], [10, 24, 0], [11, 24, 0],\n  [12, 24, 0], [13, 24, 0], [14, 24, 0], [15, 24, 0], [16, 24, 0], [17, 24, 0], [18, 24, 0],\n  [19, 24, 0], [20, 24, 0], [21, 24, 0], [22, 24, 0], [23, 24, 0], [24, 24, 0], [25, 24, 0],\n  [26, 24, 0], [27, 24, 0], [0, 23, 0], [1, 23, 0], [2, 23, 0], [3, 23, 0], [4, 23, 0], [5, 23, 0],\n  [6, 23, 0], [7, 23, 0], [8, 23, 0], [9, 23, 0], [10, 23, 0], [11, 23, 0], [12, 23, 0], [13, 23, 0],\n  [14, 23, 0], [15, 23, 0], [16, 23, 0], [17, 23, 0], [18, 23, 0], [19, 23, 0], [20, 23, 0],\n  [21, 23, 0], [22, 23, 0], [23, 23, 0], [24, 23, 0], [25, 23, 0], [26, 23, 0], [27, 23, 0],\n  [0, 22, 0], [1, 22, 0], [2, 22, 0], [3, 22, 0], [4, 22, 0], [5, 22, 0], [6, 22, 0], [7, 22, 0],\n  [8, 22, 0], [9, 22, 0], [10, 22, 0], [11, 22, 0], [12, 22, 0], [13, 22, 0], [14, 22, 0],\n  [15, 22, 0], [16, 22, 0], [17, 22, 0], [18, 22, 0], [19, 22, 0], [20, 22, 0], [21, 22, 0],\n  [22, 22, 0], [23, 22, 0], [24, 22, 0], [25, 22, 0], [26, 22, 0], [27, 22, 0], [0, 21, 0],\n  [1, 21, 0], [2, 21, 0], [3, 21, 0], [4, 21, 0], [5, 21, 0], [6, 21, 0], [7, 21, 0], [8, 21, 0],\n  [9, 21, 0], [10, 21, 0], [11, 21, 0], [12, 21, 0], [13, 21, 0], [14, 21, 0], [15, 21, 0],\n  [16, 21, 0], [17, 21, 0], [18, 21, 0], [19, 21, 0], [20, 21, 31], [21, 21, 40], [22, 21, 129],\n  [23, 21, 234], [24, 21, 234], [25, 21, 159], [26, 21, 0], [27, 21, 0], [0, 20, 0], [1, 20, 0],\n  [2, 20, 0], [3, 20, 0], [4, 20, 0], [5, 20, 0], [6, 20, 0], [7, 20, 0], [8, 20, 0], [9, 20, 0],\n  [10, 20, 0], [11, 20, 0], [12, 20, 0], [13, 20, 0], [14, 20, 0], [15, 20, 0], [16, 20, 0],\n  [17, 20, 0], [18, 20, 68], [19, 20, 150], [20, 20, 239], [21, 20, 254], [22, 20, 253],\n  [23, 20, 253], [24, 20, 253], [25, 20, 215], [26, 20, 0], [27, 20, 0], [0, 19, 0], [1, 19, 0],\n  [2, 19, 0], [3, 19, 0], [4, 19, 0], [5, 19, 0], [6, 19, 0], [7, 19, 0], [8, 19, 0], [9, 19, 0],\n  [10, 19, 0], [11, 19, 0], [12, 19, 0], [13, 19, 0], [14, 19, 0], [15, 19, 0], [16, 19, 156],\n  [17, 19, 201], [18, 19, 254], [19, 19, 254], [20, 19, 254], [21, 19, 241], [22, 19, 150],\n  [23, 19, 98], [24, 19, 8], [25, 19, 0], [26, 19, 0], [27, 19, 0], [0, 18, 0], [1, 18, 0],\n  [2, 18, 0], [3, 18, 0], [4, 18, 0], [5, 18, 0], [6, 18, 0], [7, 18, 0], [8, 18, 0], [9, 18, 0],\n  [10, 18, 0], [11, 18, 0], [12, 18, 0], [13, 18, 0], [14, 18, 19], [15, 18, 154], [16, 18, 254],\n  [17, 18, 236], [18, 18, 203], [19, 18, 83], [20, 18, 39], [21, 18, 30], [22, 18, 0], [23, 18, 0],\n  [24, 18, 0], [25, 18, 0], [26, 18, 0], [27, 18, 0], [0, 17, 0], [1, 17, 0], [2, 17, 0], [3, 17, 0],\n  [4, 17, 0], [5, 17, 0], [6, 17, 0], [7, 17, 0], [8, 17, 0], [9, 17, 0], [10, 17, 0], [11, 17, 0],\n  [12, 17, 0], [13, 17, 0], [14, 17, 144], [15, 17, 253], [16, 17, 145], [17, 17, 12], [18, 17, 0],\n  [19, 17, 0], [20, 17, 0], [21, 17, 0], [22, 17, 0], [23, 17, 0], [24, 17, 0], [25, 17, 0],\n  [26, 17, 0], [27, 17, 0], [0, 16, 0], [1, 16, 0], [2, 16, 0], [3, 16, 0], [4, 16, 0], [5, 16, 0],\n  [6, 16, 0], [7, 16, 0], [8, 16, 0], [9, 16, 0], [10, 16, 0], [11, 16, 10], [12, 16, 129],\n  [13, 16, 222], [14, 16, 78], [15, 16, 79], [16, 16, 8], [17, 16, 0], [18, 16, 0], [19, 16, 0],\n  [20, 16, 0], [21, 16, 0], [22, 16, 0], [23, 16, 0], [24, 16, 0], [25, 16, 0], [26, 16, 0],\n  [27, 16, 0], [0, 15, 0], [1, 15, 0], [2, 15, 0], [3, 15, 0], [4, 15, 0], [5, 15, 0], [6, 15, 0],\n  [7, 15, 0], [8, 15, 0], [9, 15, 0], [10, 15, 0], [11, 15, 134], [12, 15, 253], [13, 15, 167],\n  [14, 15, 8], [15, 15, 0], [16, 15, 0], [17, 15, 0], [18, 15, 0], [19, 15, 0], [20, 15, 0],\n  [21, 15, 0], [22, 15, 0], [23, 15, 0], [24, 15, 0], [25, 15, 0], [26, 15, 0], [27, 15, 0],\n  [0, 14, 0], [1, 14, 0], [2, 14, 0], [3, 14, 0], [4, 14, 0], [5, 14, 0], [6, 14, 0], [7, 14, 0],\n  [8, 14, 0], [9, 14, 0], [10, 14, 0], [11, 14, 255], [12, 14, 254], [13, 14, 78], [14, 14, 0],\n  [15, 14, 0], [16, 14, 0], [17, 14, 0], [18, 14, 0], [19, 14, 0], [20, 14, 0], [21, 14, 0],\n  [22, 14, 0], [23, 14, 0], [24, 14, 0], [25, 14, 0], [26, 14, 0], [27, 14, 0], [0, 13, 0],\n  [1, 13, 0], [2, 13, 0], [3, 13, 0], [4, 13, 0], [5, 13, 0], [6, 13, 0], [7, 13, 0], [8, 13, 0],\n  [9, 13, 0], [10, 13, 0], [11, 13, 201], [12, 13, 253], [13, 13, 226], [14, 13, 69], [15, 13, 0],\n  [16, 13, 0], [17, 13, 0], [18, 13, 0], [19, 13, 0], [20, 13, 0], [21, 13, 0], [22, 13, 0],\n  [23, 13, 0], [24, 13, 0], [25, 13, 0], [26, 13, 0], [27, 13, 0], [0, 12, 0], [1, 12, 0],\n  [2, 12, 0], [3, 12, 0], [4, 12, 0], [5, 12, 0], [6, 12, 0], [7, 12, 0], [8, 12, 55], [9, 12, 6],\n  [10, 12, 0], [11, 12, 18], [12, 12, 128], [13, 12, 253], [14, 12, 241], [15, 12, 41], [16, 12, 0],\n  [17, 12, 0], [18, 12, 0], [19, 12, 0], [20, 12, 0], [21, 12, 0], [22, 12, 0], [23, 12, 0],\n  [24, 12, 0], [25, 12, 0], [26, 12, 0], [27, 12, 0], [0, 11, 0], [1, 11, 0], [2, 11, 0], [3, 11, 0],\n  [4, 11, 0], [5, 11, 0], [6, 11, 25], [7, 11, 205], [8, 11, 235], [9, 11, 92], [10, 11, 0],\n  [11, 11, 0], [12, 11, 20], [13, 11, 253], [14, 11, 253], [15, 11, 58], [16, 11, 0], [17, 11, 0],\n  [18, 11, 0], [19, 11, 0], [20, 11, 0], [21, 11, 0], [22, 11, 0], [23, 11, 0], [24, 11, 0],\n  [25, 11, 0], [26, 11, 0], [27, 11, 0], [0, 10, 0], [1, 10, 0], [2, 10, 0], [3, 10, 0], [4, 10, 0],\n  [5, 10, 0], [6, 10, 231], [7, 10, 245], [8, 10, 108], [9, 10, 0], [10, 10, 0], [11, 10, 0],\n  [12, 10, 132], [13, 10, 253], [14, 10, 185], [15, 10, 14], [16, 10, 0], [17, 10, 0], [18, 10, 0],\n  [19, 10, 0], [20, 10, 0], [21, 10, 0], [22, 10, 0], [23, 10, 0], [24, 10, 0], [25, 10, 0],\n  [26, 10, 0], [27, 10, 0], [0, 9, 0], [1, 9, 0], [2, 9, 0], [3, 9, 0], [4, 9, 0], [5, 9, 0],\n  [6, 9, 121], [7, 9, 245], [8, 9, 254], [9, 9, 254], [10, 9, 254], [11, 9, 217], [12, 9, 254],\n  [13, 9, 223], [14, 9, 50], [15, 9, 0], [16, 9, 0], [17, 9, 0], [18, 9, 0], [19, 9, 0], [20, 9, 0],\n  [21, 9, 0], [22, 9, 0], [23, 9, 0], [24, 9, 0], [25, 9, 0], [26, 9, 0], [27, 9, 0], [0, 8, 0],\n  [1, 8, 0], [2, 8, 0], [3, 8, 0], [4, 8, 0], [5, 8, 0], [6, 8, 0], [7, 8, 116], [8, 8, 165],\n  [9, 8, 233], [10, 8, 233], [11, 8, 234], [12, 8, 180], [13, 8, 39], [14, 8, 3], [15, 8, 0],\n  [16, 8, 0], [17, 8, 0], [18, 8, 0], [19, 8, 0], [20, 8, 0], [21, 8, 0], [22, 8, 0], [23, 8, 0],\n  [24, 8, 0], [25, 8, 0], [26, 8, 0], [27, 8, 0], [0, 7, 0], [1, 7, 0], [2, 7, 0], [3, 7, 0],\n  [4, 7, 0], [5, 7, 0], [6, 7, 0], [7, 7, 0], [8, 7, 0], [9, 7, 0], [10, 7, 0], [11, 7, 0],\n  [12, 7, 0], [13, 7, 0], [14, 7, 0], [15, 7, 0], [16, 7, 0], [17, 7, 0], [18, 7, 0], [19, 7, 0],\n  [20, 7, 0], [21, 7, 0], [22, 7, 0], [23, 7, 0], [24, 7, 0], [25, 7, 0], [26, 7, 0], [27, 7, 0],\n  [0, 6, 0], [1, 6, 0], [2, 6, 0], [3, 6, 0], [4, 6, 0], [5, 6, 0], [6, 6, 0], [7, 6, 0], [8, 6, 0],\n  [9, 6, 0], [10, 6, 0], [11, 6, 0], [12, 6, 0], [13, 6, 0], [14, 6, 0], [15, 6, 0], [16, 6, 0],\n  [17, 6, 0], [18, 6, 0], [19, 6, 0], [20, 6, 0], [21, 6, 0], [22, 6, 0], [23, 6, 0], [24, 6, 0],\n  [25, 6, 0], [26, 6, 0], [27, 6, 0], [0, 5, 0], [1, 5, 0], [2, 5, 0], [3, 5, 0], [4, 5, 0],\n  [5, 5, 0], [6, 5, 0], [7, 5, 0], [8, 5, 0], [9, 5, 0], [10, 5, 0], [11, 5, 0], [12, 5, 0],\n  [13, 5, 0], [14, 5, 0], [15, 5, 0], [16, 5, 0], [17, 5, 0], [18, 5, 0], [19, 5, 0], [20, 5, 0],\n  [21, 5, 0], [22, 5, 0], [23, 5, 0], [24, 5, 0], [25, 5, 0], [26, 5, 0], [27, 5, 0], [0, 4, 0],\n  [1, 4, 0], [2, 4, 0], [3, 4, 0], [4, 4, 0], [5, 4, 0], [6, 4, 0], [7, 4, 0], [8, 4, 0], [9, 4, 0],\n  [10, 4, 0], [11, 4, 0], [12, 4, 0], [13, 4, 0], [14, 4, 0], [15, 4, 0], [16, 4, 0], [17, 4, 0],\n  [18, 4, 0], [19, 4, 0], [20, 4, 0], [21, 4, 0], [22, 4, 0], [23, 4, 0], [24, 4, 0], [25, 4, 0],\n  [26, 4, 0], [27, 4, 0], [0, 3, 0], [1, 3, 0], [2, 3, 0], [3, 3, 0], [4, 3, 0], [5, 3, 0],\n  [6, 3, 0], [7, 3, 0], [8, 3, 0], [9, 3, 0], [10, 3, 0], [11, 3, 0], [12, 3, 0], [13, 3, 0],\n  [14, 3, 0], [15, 3, 0], [16, 3, 0], [17, 3, 0], [18, 3, 0], [19, 3, 0], [20, 3, 0], [21, 3, 0],\n  [22, 3, 0], [23, 3, 0], [24, 3, 0], [25, 3, 0], [26, 3, 0], [27, 3, 0], [0, 2, 0], [1, 2, 0],\n  [2, 2, 0], [3, 2, 0], [4, 2, 0], [5, 2, 0], [6, 2, 0], [7, 2, 0], [8, 2, 0], [9, 2, 0], [10, 2, 0],\n  [11, 2, 0], [12, 2, 0], [13, 2, 0], [14, 2, 0], [15, 2, 0], [16, 2, 0], [17, 2, 0], [18, 2, 0],\n  [19, 2, 0], [20, 2, 0], [21, 2, 0], [22, 2, 0], [23, 2, 0], [24, 2, 0], [25, 2, 0], [26, 2, 0],\n  [27, 2, 0], [0, 1, 0], [1, 1, 0], [2, 1, 0], [3, 1, 0], [4, 1, 0], [5, 1, 0], [6, 1, 0], [7, 1, 0],\n  [8, 1, 0], [9, 1, 0], [10, 1, 0], [11, 1, 0], [12, 1, 0], [13, 1, 0], [14, 1, 0], [15, 1, 0],\n  [16, 1, 0], [17, 1, 0], [18, 1, 0], [19, 1, 0], [20, 1, 0], [21, 1, 0], [22, 1, 0], [23, 1, 0],\n  [24, 1, 0], [25, 1, 0], [26, 1, 0], [27, 1, 0]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v = new BrunelVis('visidebf5877f-a389-405b-ab2a-4cb525d71071');\nv.build(table1);\n\n            \"\"\n        });\n        });\n        </script>"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "%%brunel data('pixelsDF') x(x) y(y) color(color) :: width=400, height=400  ", 
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Pipeline\nLet's write the ML model, training and test as an ML Pipeline", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Define", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\n\n// specify layers for the neural network:\n// input layer of size 28*28 (features), \n// intermediate of size 300\n// and output of size 10 (classes)\nval layers = Array[Int](784, 300, 10)\n// create the trainer and set its parameters\nval MLPtrainer = new MultilayerPerceptronClassifier().\n  setLayers(layers).\n  setBlockSize(128).\n  setSeed(1234L).\n  setMaxIter(100)\n\nval MLPpipeline = new Pipeline().setStages(Array(MLPtrainer))", 
            "execution_count": 38, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "println(MLPtrainer.explainParams())", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Train", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "// train the model\nval MLPmodel = MLPpipeline.fit(trainData)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "### Test", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "// compute accuracy on the test set\nval MLPresult = MLPmodel.transform(testData)\nval MLPpredictionAndLabels = MLPresult.select(\"prediction\", \"label\")\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval MLPevaluator = new MulticlassClassificationEvaluator().setMetricName(\"precision\")//accuracy\")\nprintln(\"Accuracy: \" + MLPevaluator.evaluate(MLPpredictionAndLabels))", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Display one of the results", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "MLPresult.show(5)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "val resultPixelDF = previewRecord(MLPresult.select(\"prediction\",\"features\"),2)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "%%brunel data('resultPixelDF') x(x) y(y) color(color) :: width=400, height=400  ", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Build an hyperparameter grid, train the models, and display the best parameters", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n\nval MLPparamGrid = new ParamGridBuilder().\n  addGrid(MLPtrainer.layers, Array(Array(784, 30, 10), Array(784, 150, 10), Array(784, 300, 10))).\n  addGrid(MLPtrainer.maxIter, Array(10,100,200)).\n  build()\n  \nval MLPcrossValidator = new CrossValidator().\n  setEstimator(MLPpipeline).\n  setEvaluator(new MulticlassClassificationEvaluator).\n  setEstimatorParamMaps(MLPparamGrid).\n  setNumFolds(2)  // Use 3+ in practice", 
            "execution_count": 130, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "// Run cross-validation, and choose the best set of parameters.\nval MLPcvModel = MLPcrossValidator.fit(trainData)", 
            "execution_count": 131, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "{\n\tmlpc_e8926a20385b-layers: [I@79da7bde,\n\tmlpc_e8926a20385b-maxIter: 200\n}\n"
                }
            ], 
            "source": "val bestParams = MLPcvModel.getEstimatorParamMaps.zip(MLPcvModel.avgMetrics).maxBy(_._2)._1\nprintln(bestParams)", 
            "execution_count": 246, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "featuresCol: features column name (default: features)\nlabelCol: label column name (default: label)\npredictionCol: prediction column name (default: prediction)\n"
                }
            ], 
            "source": "import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel\nval MLPbestPipeline = MLPcvModel.bestModel.asInstanceOf[PipelineModel]\nval MLPbestModel = MLPbestPipeline.stages(0).asInstanceOf[MultilayerPerceptronClassificationModel]\nprintln(MLPbestModel.explainParams)", 
            "execution_count": 247, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat_minor": 0
}